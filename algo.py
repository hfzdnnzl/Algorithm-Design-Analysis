# -*- coding: utf-8 -*-
"""Algo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11us1NlG7lTPt1AcM9dhSwjnO6t1WJYQq
"""

#@title Load special libraries {display-mode: "form"}
 
!pip install -q newspaper3k
!pip install -q gmplot

#@title Load data {display-mode: "form"}
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import os
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
def downloadFromDrive(drive_file,path="/"):
  downloaded = drive.CreateFile({'id': drive_file["id"]})
  filename = path+drive_file["title"]
  os.makedirs(os.path.dirname(filename), exist_ok=True)
  open(path+drive_file["title"],"w+").write(downloaded.GetContentString())
 
folder_id = "1aam4VtUP5QkKvSFbtCv5R-EDCjnKz5Q6"
file_list = drive.ListFile({'q': "'{}' in parents and trashed=false".format(folder_id)}).GetList()
 
for x in file_list:
  downloadFromDrive(x,"res/")

#@title Import Statements {display-mode: "form"}
import pandas as pd
pd.set_option('max_colwidth', 1000)

from geopy.geocoders import Nominatim
from geopy.distance import geodesic,great_circle
from geopy.exc import GeocoderTimedOut
import itertools
from collections import defaultdict



import urllib.request
import urllib.parse
import re
import collections 
from collections import Counter
from newspaper import Article

"""# Problem 1
Ben Sherman is a UK broker who is looking for industrial investment opportunities in the cities of Asia. He already invested in a company in Kuala Lumpur and now he plans to travel to several cities in Asia from Kuala Lumpur to expand his investment. The cities include Jakarta, Bangkok, Taipei, Hong Kong, Tokyo, Beijing and Seoul.

**Get and mark locations of the all the cities Ben plan to visit**

a.	Guide 1: you can use Python Geocoding Toolbox

Look up: https://pypi.python.org/pypi/geopy#downloads

b.	Guide 2: you can use gmplot
     
Lookup: https://github.com/vgm64/gmplot
"""

geolocator = Nominatim(user_agent="ALGO")

lStr = ["Kuala Lumpur","Jakarta","Bangkok","Taipei","Hong Kong","Tokyo","Beijing","Seoul"]
lDict = {}
for i in range(len(lStr)):
  lDict[lStr[i]] = i
locations = {}
for x in lStr:
    location = geolocator.geocode(x)
    locations[x] = location
print(locations)

"""**Get the distances between each of these destinations**

a.	Guide 1: you can use Python Geocoding Toolbox

b.	Suggestion 2: you should use Google Distance Matrix API

>i.	Login to the google developerâ€™s website and follow through the examples. It is important that you know how to use the API key given to you within the code that you are going to use. 
>>Refer to this link: https://developers.google.com/maps/documentation/distance-matrix/start
"""

# distances = defaultdict(dict)
distanceTable = []
for i in range(len(locations)):
    distances = []
    x = locations[lStr[i]]
    # print("{:20s}".format(lStr[i]),end="\t")
    for j in range(len(locations)):
        y = locations[lStr[j]]
        s = float(geodesic((x.latitude, x.longitude),(y.latitude,y.longitude)).km)
        distances.append(s)
        # distances[lStr[i]][lStr[j]]=s
        # print("{:2.2f}".format(s),end="\t")
    distanceTable.append(distances)

distDF = pd.DataFrame(distanceTable)
display(distDF)

def getDistance(x,y):
  return distanceTable[lDict[x]][lDict[y]]

"""**Journey planner:** 

Suggest a journey for Ben to visit each of the cities once with the least distance travelled.
"""

start_point = lStr.pop(0)
perm = list(itertools.permutations(lStr))
lStr = [start_point]+ lStr

minimum = 100000000000000000
bestSeq = []
for seq in perm:#n
    seq = list(seq)
    distance = 0
    org = start_point
    for turn in seq:#n O(n^2)
        distance+=getDistance(org,turn) #this code keeps calculating distance from the first point
        org = turn
        if(distance>minimum):
            break
    if(distance<minimum):
        minimum = distance
        bestSeq=seq
bestSeq = [start_point] +bestSeq

print("best sequence: ",bestSeq)
print("Total of Shortest Distance: ", minimum)
print("Coordinates of Best Sequence")
for x in bestSeq:
    print(x,": ",locations[x].latitude,locations[x].longitude)
print()

"""**Plot line between the destinations.**

a.	Guide1:  you can use google.maps.Polyline. 
>You can refer to this link:
https://www.sitepoint.com/create-a-polyline-using-the-geolocation-and-the-google-maps-api/
"""

import gmplot
lat,lon = [locations[location].latitude for location in locations],[locations[location].longitude for location in locations]
gmap = gmplot.GoogleMapPlotter(lat[0],lon[0],4,apikey="AIzaSyBdil_LOkqcOcCDqrwKx-VzL14rUQb8OIo")
gmap.scatter(lat,lon,'#ff0000',size=50,marker=False)
gmap.plot(lat,lon,'blue',edge_width=2.5)
gmap.draw("map.html")

for x in lStr:
    print()
    print(x,"\n",locations[x].address)
    print(location.latitude,location.longitude)
    print()
    
for x in lStr:
    i=1
    for y in lStr:
        s = getDistance(x,y)
        if i==1:
            print()
            print("    ",x)
            print("------------------------------")
            i+=1
        if s==0:
            continue
        else:
            print("{:9.2f}".format(s),"km to",y)
            print()

"""# Problem 2
Ben decided to focus more on the possibilities of better return of investment in cities which has a positive economy and financial growth. So, Ben needs to do some analysis of local economy and finance situation for the last 3 months.
"""

class Country:
  def __init__(self, name, urls):
    self.name = name
    self.location = geolocator.geocode(name)
    self.urls = urls
 
    self.articles = []
 
    self.words = []
 
    self.real_words = []
    self.non_words = []
    
    self.count_real_words = []
    self.count_non_words = []
 
 
    self.positives = []
    self.negatives = []
 
    self.sentiment = []
    self.wordfreq = []
    self.wordfreqpos = []
    self.wordfreqneg = []
 
  
  def loadArticleFromURL(self):
    s = ""
    for url in self.urls:
      # try:
        temp = Article(url)
        temp.download()
        temp.parse()
        # print(temp.text)
        s+= " "+ temp.text
      # except:
      #   print("error at ", url)
      #   continue
       # self.articles.append(temp.text)
    pattern = re.compile('[\W_]+')
 
    s = re.sub(pattern," ",s)
 
    self.articles.append(s.lower())
 
 
  def splitArticles(self):
    for article in self.articles:
      self.words.append(article.split())
    
  def removeStopWords(self,stop_words):
    for word in self.words:
      real_word, non_word = [], []
      for wor in word:
        if wor in stop_words:
          non_word.append(wor)
        else:
          real_word.append(wor)
 
      self.real_words.append(real_word)
      self.non_words.append(non_word)
 
  def createCounters(self):
    for real_word in self.real_words:
      self.count_real_words.append(Counter(real_word))
    for non_word in self.non_words:
      self.count_non_words.append(Counter(non_word))
 
  def getPolarity(self):
    self.sentiment = []
    with open("res/positive.txt") as f:
      positiveWords = f.readlines()
    with open("res/negative.txt") as f:
      negativeWords = f.readlines()
 
    positiveWords = [x.replace("\n","") for x in positiveWords]
 
    negativeWords = [x.replace("\n","") for x in negativeWords]
 
    for word in self.real_words[0]:
      if word in positiveWords:
        self.positives.append(word)
        self.wordfreqpos.append(self.positives.count(word))
        self.wordfreq = list(zip(self.positives,self.wordfreqpos))
      elif word in negativeWords:
        self.negatives.append(word)
        self.wordfreqneg.append(self.negatives.count(word))
        self.wordfreq = list(zip(self.negatives,self.wordfreqneg))
    self.sentiment.append(((len(self.positives)-len(self.negatives))/len(self.real_words[0])))

"""**Extract information from major economic/financial news website for each city. You need to find 5 related articles within the last 3 months to be analysed.**

a.	Sometimes a webpage must be converted to the text version before it can be done

>i.	Guide 1: You may refer to this website to extract word from a website
https://www.textise.net/ 

b.	Guide 2: You may refer to this website on how to count word frequency in a website
https://programminghistorian.org/lessons/counting-frequencies 

c.	You can also filter stops words from the text you found

>i.	Guide 3: Stops words are such as conjunctions and prepositions. You may refer to this link: https://www.ranks.nl/stopwords 
>
>ii.	Program using Rabin-karp algorithm to find and delete the stop words.
"""

countries = {
 "Kuala Lumpur" :Country("Kuala Lumpur", [
 'https://www.aljazeera.com/news/2019/12/kl-summit-major-issues-facing-muslim-world-191217053320764.html',
 'https://www.nst.com.my/news/nation/2020/03/577791/additional-stimulus-package-must-address-economic-sustainability',
 'https://www.theedgemarkets.com/article/extraordinary-times-require-extraordinary-economic-actions',
 'https://www.nst.com.my/business/2020/04/584545/malaysia-more-resilient-many-countries-world-bank',
 'https://www.channelnewsasia.com/news/asia/malaysia-economy-could-shrink-more-than-earlier-forecast-covid19-12697044'
 
 ]),
 
 "Jakarta" :Country("Jakarta", [
 "https://www.thejakartapost.com/news/2020/03/05/indonesias-domestically-driven-economy-could-be-blessing-in-disguise-amid-outbreak-adb.html",
 'https://www.thejakartapost.com/news/2020/03/19/indonesias-gdp-to-grow-4-8-this-year-as-covid-19-shakes-economy-fitch-solutions.html',
 'https://www.thejakartapost.com/news/2020/04/06/adb-projects-indonesias-economy-to-grow-2-5-in-2020.html',
 'https://www.thejakartapost.com/news/2020/04/14/imf-projects-0-5-growth-for-indonesia-as-global-economy-faces-deep-recession.html',
 'https://www.thejakartapost.com/news/2020/05/06/indonesias-economy-heads-into-turbulence-as-q1-growth-plunges-economists.html'
 
 ]),
 "Bangkok" :Country("Bangkok", [
 'https://www.bangkokpost.com/thailand/general/1872529/poverty-on-the-rise-again-as-economy-slows',
 'https://www.bangkokpost.com/business/1882650/pandemic-sinks-economy',
 'https://www.economist.com/asia/2020/04/02/thailands-economy-was-suffering-before-the-virus',
 'https://www.bangkokpost.com/business/1900795/imf-thai-gdp-down-6-7-',
 'https://www.bangkokpost.com/thailand/general/1913088/virus-toll-on-economy-to-linger'
 ]),
 "Taipei" :Country("Taipei", [
 "https://www.focus-economics.com/countries/taiwan/news/monetary-policy/central-bank-holds-rates-at-record-low-level-in-june",
 'https://focustaiwan.tw/politics/202003250020',
 'https://fee.org/articles/why-taiwan-hasnt-shut-down-its-economy/',
 'https://www.ft.com/content/b59c238c-d004-44a2-bd9f-c5b1e7a5bc8a',
 'https://www.taiwannews.com.tw/en/news/3930309'
 ]),
 "Hong Kong" :Country("Hong Kong", [
 'https://thediplomat.com/2020/03/helping-hong-kongs-economy-weather-protests-and-coronavirus/',
 'https://www.bloomberg.com/news/articles/2020-03-22/hong-kong-s-economy-is-in-deep-water-finance-chief-says',
 'https://www.ft.com/content/4d828bd6-8f98-47a4-9f22-61045f73e95d',
 'https://www.cnbc.com/2020/04/14/coronavirus-hong-kong-economy-after-protests-trade-war-and-pandemic.html',
 'https://edition.cnn.com/2020/05/04/economy/hong-kong-q1-gdp-economy-coronavirus/index.html'
 ]),
 "Tokyo" :Country("Tokyo", [
 'https://www.japantimes.co.jp/news/2020/03/07/business/economy-business/canceling-tokyo-olympics-gdp/#.XuG3iUUzbcc',
 'https://www.cnbc.com/2020/03/23/analysts-on-economic-impact-of-postponing-olympics-2020-in-tokyo-japan.html',
 'https://www.nytimes.com/2020/03/30/business/japan-economy-coronavirus.html',
 'https://www.tokyoreview.net/2020/04/covid-19-japan-economy-breaking-point/',
 'https://www.worldpoliticsreview.com/trend-lines/28748/the-japan-olympics-are-in-danger-as-the-economy-reels-from-covid-19'
 ]),
 "Beijing" :Country("Beijing", [
 'https://www.greenbiz.com/article/chinas-climate-progress-may-have-faltered-2018-it-seems-be-right-path',
 'https://www.aljazeera.com/news/2020/06/live-india-china-armies-hold-talks-disputed-ladakh-region-200618062508784.html',
 'https://www.nytimes.com/2020/03/12/business/china-coronavirus-economy.html',
 'https://www.cnbc.com/2020/03/20/coronavirus-hits-chinas-economy-twice-as-financial-contagion-spreads-across-the-globe.html',
 'https://www.nytimes.com/2020/04/09/business/economy/coronavirus-china-economy-stimulus.html'
 
 ]),
 "Seoul" :Country("Seoul", [
 'http://www.koreaherald.com/view.php?ud=20200309000581',
 'https://asiatimes.com/2020/03/seoul-loosens-purse-strings-as-economy-takes-hit/',
 'https://www.wsj.com/articles/south-korea-tamps-down-coronavirus-but-economy-remains-paralyzed-11586180838',
 'https://www.reuters.com/article/us-southkorea-economy-gdp-poll/south-korea-set-to-post-largest-gdp-contraction-since-2008-reuters-poll-idUSKBN2230IE',
 'https://www.straitstimes.com/business/economy/south-koreas-april-factory-activity-plunges-as-export-orders-shrink-at-record-pace'
 ]),
}
 
for country in countries:
  countries[country].loadArticleFromURL()
  countries[country].splitArticles()

stop_words = ["","s"]
with open("res/trash.txt") as f:
    for line in f:
        stop_words.append(line.rstrip())
 
for country in countries:
  countries[country].removeStopWords(stop_words)

"""**Plot line/scatter/histogram graphs related to the word count using Plotly (Word count, stop words)**

d.	Guide 3: You may refer this link on how to install Plotly and how to use the API keys
 http://www.instructables.com/id/Plotly-with-Python/ 
https://plot.ly/python/getting-started/
"""

for country in countries:
  countries[country].getPolarity()
  countries[country].createCounters()

# OUTPUT
 
for country in countries:
  print("Country: " +country)
  print(" Total words: " + str(len(countries[country].words[0])))
  print(" Total stop words: " + str(len(countries[country].non_words[0])))
  print(" Total real words: " + str(len(countries[country].real_words[0])))

for country in countries:
 
  print("Country: " + country)
  data = {
  "Country : ": [country for country in countries],
  "Positive words" :[countries[country].positives for country in countries],
  "Frequency of Positive word":[countries[country].wordfreqpos for country in countries],
  "Total count of positive words: ":[len(countries[country].positives)for country in countries],
  "Negative words" :[countries[country].negatives for country in countries],
  "Frequency of Negative word":[countries[country].wordfreqneg for country in countries],
  "Total count of negative words: ":[len(countries[country].negatives)for country in countries]
  
} 
  df = pd.DataFrame(data)
  df

df

for i in range(len(countries)):
  temp = pd.DataFrame(df.iloc[i,:])
  temp.style.set_properties(**{"width":"1300px"})
  display(temp)
  print()

for country in countries:
    print("Country: " + country)
    print("Sentiment: " + str(countries[country].sentiment[0]*100) + " %")

"""**Compare words in the webpages with the positive, negative and neutral English words using a String-Matching algorithm**

e.	Guide 4: Use the following word as positive and negative English words
>http://positivewordsresearch.com/list-of-positive-words/
>
>http://positivewordsresearch.com/list-of-negative-words/ 

f.	Put these words in a text file for you to access them in your algorithm

g.	Words that are not in the list can be considered as neutral

**Plot histogram graphs of positive and negative words found in the webpages.**

h.	Guide 5: Use Plotly
"""

# import plotly.offline as plo
import plotly.graph_objects as go
cities =list(countries.keys())
tempReal = [len(countries[country].real_words[0])for country in countries]
tempStop = [len(countries[country].non_words[0]) for country in countries]
fig = go.Figure(data=[
    go.Bar(name='Real words', x=cities, y=tempReal, text=tempReal,textposition="auto"),
    go.Bar(name='Stop words', x=cities, y=tempStop, text=tempStop,textposition="auto"),
    ])
# Change the bar mode
fig.update_layout(barmode='stack')
fig.show()

"""**Give an algorithmic conclusion regarding the sentiment of those articles**

i.	Guide 6: If there are more positive words, conclude that the article is giving positive sentiment, if there are more negative words, conclude that the article is giving negative sentiment.

j.	You may try to conclude in different perspectives such as whether the list of positive and negative words above is accurate to be used in the context of the article you extracted the text.

k.	Based on the conclusion, you may say the country has positive or negative economic/financial situation.
"""

cities = list(countries.keys())
tempPos = [len(countries[country].positives) for country in countries]
tempNeg = [len(countries[country].negatives) for country in countries]
fig = go.Figure(data=[
    go.Bar(name='Positive Words', x=cities, y=tempPos,text=tempPos,textposition="auto"),
    # go.Bar(name='Negative Words', x=cities, y=[len(countries[country].negatives) for country in countries],base=[(-1*len(countries[country].negatives)) for country in countries])
    go.Bar(name='Negative Words', x=cities, y=tempNeg,text=tempNeg,textposition="auto")
])
 
# fig.update_layout(barmode='stack')
fig.show()

cities = list(countries.keys())
temp =[countries[country].sentiment[0] for country in countries]
fig = go.Figure(data=[
    go.Bar(name='Polarity', x=cities, y=temp, text=["{:.2f}%".format(t*100) for t in temp], textposition="auto")
      ]
)
# Change the bar mode
fig.update_layout(barmode='group', yaxis={"tickformat": '.1%'})
fig.show()

"""# Problem 3
Ben realised that he needs to optimise his travel. He will give priority to cities with possible better investment return based on the analysis of local economic and financial situation. If next nearest city to be visited have less better economic and financial situation than any of the other cities, Ben will visit other city first provided that the difference of distance between the 2 cities is not more than 40% and the difference of sentiment analysis between the 2 cities is not less than 2%.

**Calculate the total probability distribution of possible routes. Then, write the summary of all possible route for Ben to take, ranking from the most recommended to the least recommended.**
"""

#@title Upload to Drive {display-mode: "form"}
folder_id = "1V6z1L-Btw3uN8SlPPVYyrXVuo6po06Zu"
 
for dirpath, dirnames, files in os.walk('./out'):
    for file_name in files:
        # print(file_name)
        file1 = drive.CreateFile({'parents': [{'id': folder_id}],"title":file_name})
        file1.SetContentFile(dirpath+"/"+file_name)
        file1.Upload()

class Node:
  def __init__(self,l,n,d):
    self.li = list(l)
    self.endOfPath=False
    self.distances = []
    self.totalTravelled = d
    self.closestChild=None
    if(len(self.li)>0):
      self.name = self.li.pop(n)
      self.country = countries[self.name]
      self.location  = locations[self.name]
      self.children = {}
      for x in range(len(self.li)):
        if self.closestChild==None:
          self.closestChild = self.li[x]
        elif getDistance(self.name,self.closestChild)>getDistance(self.name,self.li[x]):
          self.closestChild = self.li[x]
        curD = d+self.getWeighted(self.li[x])
        self.children[self.li[x]] = Node(self.li,x,curD)
        self.distances.append(curD)
        # self.children.append(Node(self.li,x,curD))
    if self.children == {}:
      self.endOfPath = True


  def getWeighted(self,dest):

    #distance to next country
    closestDistance = getDistance(self.name,self.closestChild)
    destDistance = getDistance(self.name, dest)

    #different sentiment of two next country
    destSentiment = countries[dest].sentiment
    closestSentiment = countries[self.closestChild].sentiment
    #dif KL -> jakarta (-1.81-1.98: worse??) 
    #fatin=-fatin
    fatin = (destSentiment[0]-closestSentiment[0])

    #different distance of two next country
    asfa=((destDistance-closestDistance)/closestDistance)*100.00
    #amir.... 
    print("Current Country: "+self.name)
    print("Closest destination: "+self.closestChild)
    print("Destination to go: "+dest)
    print("Sentiment Difference: ",fatin)
    print("Distance Difference: ",asfa)
    weight = 1
    if(fatin<=2.0):
      print("Difference in sentiment not significant,",dest,"is not better.")
    else:
      print("Difference in sentiment is significant. Checking difference in distance to",dest)
      if(asfa<=40):
        print("Difference in distance to",dest,"is less than 40%. Prioritise",dest)
        weight = 0.5
      else:
        print("Difference in distance to",dest,"is more than 40%.",dest,"is not better.")
    return destDistance*weight

  def __str__(self):
    return str(self.name)

  def printAll(self,s):
    # print(self.endOfPath)
    if self.endOfPath:
      print(s+"->"+ self.name + " Total travelled: "+str(self.totalTravelled))
    for child in self.children:
      child.printAll(s+("->" if s !="" else "")+self.name)

  def arrayAll(self,s,master):
    # print(self.endOfPath)
    if self.endOfPath:
      master.append([s+[self.name],self.totalTravelled])
    for child in self.children:
      self.children[child].arrayAll(s+[self.name],master)
  def bestDest():
    return

print(lStr)

root = Node(lStr,0,0)
# print(root.printAll(""))
arrayAll = []
root.arrayAll([],arrayAll)

print("Shortest Distance")
print(bestSeq)
#Sort array
arrayAll.sort(key=lambda x:x[1])

print(arrayAll)

all = pd.DataFrame(
    {
        "Path": [i[0] for i in arrayAll],
        "Distance": [i[1] for i in arrayAll]
     
    }
)

bottom = pd.DataFrame(all.iloc[-5:])
top = pd.DataFrame(all.iloc[:5])
# top.style.set_properties(**{"width":"1000px"})
# display(top.style.set_properties(**{"width":"1000px"}))
# bottom.style.set_properties(subset=["Path"],**{"width":"700px"})
# display(bottom.style.set_properties(**{"width":"1000px"}))

display(top)
display(bottom)